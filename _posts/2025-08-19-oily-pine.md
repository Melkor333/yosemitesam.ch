---
title: Oily Pine
post:
  excerpt: "Build all Alpine Linux packages with OSH from Oils for Unix as shell"
---

I want to share a project Iâ€™ve been working on over the last year - I call it `Oily Pine`. The name is just a mix of Oils and Alpine. :) 

**TLDR; Build all [Alpine Linux](https://www.alpinelinux.org/) packages with OSH from [Oils for Unix](https://oils.pub/) as shell. Sounds easy, right?**

## Introduction

Osh is probably the newest POSIX compliant shell out there. Itâ€™s both POSIX *and* Bash compatible, and has some flags to remove "classical" shell syntax/features in favor of much better designed alternatives that make it a modern shell.

The Bash/POSIX compatible part is already very mature. Osh runs [real shell programs](https://github.com/oils-for-unix/oils/wiki/Shell-Programs-That-Run-Under-OSH) since 2017. And the Oils repo has a *lot* of tests, benchmarks and metrics which are published on each release ([latest](https://oils.pub/release/latest/quality.html)).

OSH can run some of the biggest publicly available shell scripts. But is this enough? Especially with shell my experience is that people typically learn a unique style of writing their code and then stick to it. Why try out a feature which could shoot you in the foot, when you can achieve the same behaviour with 5 messy lines of code you know will work when you copy them from your last script, right?
This became very apparent to me when I went through around 50 Nagios checks from "the internet" with my apprentice last year and tried to apply shellcheck to them. We saw vastly different styles and had to use an unexpected amount of `#shellcheck disable=...`!

So I believe we need to test *many different* shell scripts, rather than a few big codebases! It will happen automatically when people start adopting it, but until then we need to do that work ourselves.

### Searching for Shell Scripts

Shell is a sneaky language. Just recently, I learned that the line `VirusEvent mycommand` in a [Clamav](https://www.clamav.net/) config file will execute the command `$USER_SHELL -c 'mycommand'` when it finds a Virus. Not just `mycommand`. This means I could also do something crazy like `VirusEvent for i in 1 2 3; do echo $i; done; echo "VIRUS!"` (as long as my shell supports this)! Shell is "embedded" like that in many places. Most prominently in `Makefile`s:

```make
mytarget:
	echo this is shell

secondtarget:
	# We can do the same thing as in the Clamav VirusEvent
	for i in a b c; do echo $1; done
```

Therefore, itâ€™s not a far stretch to start *building packages* when we want to run a lot of shell, because that's usually done with `Makefile`s.
Not only are there `Makefiles`, but also huge `configure` scripts written in shell. And Alpine Linux "Aports" are even *package definitions* written in shell! And it turns out there are testing tools written in shell, too!
So weâ€™ll definitely get a few different styles of shell when.

And thatâ€™s why I started that project to build all Aports using Oils as build shell.

| Note: The idea is NOT to make Oils the default shell in Alpine Linux. Alpine is as minimal as possible, and in that regard Oils is no competition against the built-in `busybox ash`. But package definitions and a lot of tooling for Alpine is written in shell, which makes it a great target for testing a shell. Itâ€™s also much easier to bootstrap than most other distributions - which would make heavy use of shell as well.<br /><br />And as a Swiss person, I like Mountains. :p |

## The Repository

I started the `oily-pine` project by forking the official [Aports repo](https://gitlab.alpinelinux.org/alpine/aports/-/tree/master?ref_type=heads), which contains all the package definitions of the distribution. To make eventual rebasing easy, I created a folder `oily` in the repo and put everything in there. If youâ€™re curious, [check it out](https://github.com/Melkor333/oily-pine/tree/master/oily) after finishing this post. :)

### Containers

To build Alpine packages, we need to set up a build environment. There are a few build environment options for that:
- A virtual machine with Alpine Linux
- An Alpine container image
- A chroot

When I began the project, I thought using a container is probably the most straight forward way. So thatâ€™s what I did.

To build a container image, we need a `Containerfile`. It's literally the same as a `Dockerfile`, but I use `podman` instead of Docker:

```dockerfile
# oily/container/Containerfile

from alpine:latest

ADD setup-build-env.sh /bin/setup-build-env.sh
RUN setup-build-env.sh

USER packager
WORKDIR /home/packager
VOLUME ["/home/packager/aports"]
VOLUME ["/home/packager/.abuild"]
VOLUME ["/home/packager/logs"]
VOLUME ["/home/packager/packages"]
ENTRYPOINT ["/home/packager/aports/oily/container/package.sh"]
```

This file does not do much:
- Start from Alpine linux base container
- Run the `setup-build-env.sh`
- Set some variables on how a container with this image should be executed

The hard work is in the `setup-build-env.sh`. It downloads and installs Oils (which is in the testing repository), creates a `packager` user, gives it the required permissions and generates a signing-key:

```shell
# oily/container/setup-build-env.sh

# oils-* is only in the testing repos...
echo https://dl-cdn.alpinelinux.org/alpine/edge/testing >> /etc/apk/repositories
apk add abuild abuild-rootbld doas build-base alpine-sdk oils-for-unix oils-for-unix-binsh oils-for-unix-bash lua-aports
# Delete the testing repo again
sed -i '/testing/d' /etc/apk/repositories

# wheel group should be able to run `doas` without password
echo 'permit nopass :wheel' > /etc/doas.conf

adduser -Du 1000 packager
adduser packager abuild
adduser packager wheel
su packager -c "abuild-keygen -na --install"
mkdir -p /home/packager/aports
mkdir -p /home/packager/packages
chown -R packager /home/packager

# Remove the cache of installed packages to make the container smaller
apk cache clean --purge
```

### The Oils Package

My script installs 3 packages:
- oils-for-unix
- oils-for-unix-binsh
- oils-for-unix-bash

This is because I was very considerate when creating the [aport definition](https://gitlab.alpinelinux.org/alpine/aports/-/merge_requests/70331):

The package `oils-for-unix` creates a binary `/usr/bin/oils-for-unix` and 2 symlinks `/usr/bin/osh` and `/usr/bin/ysh`, both pointing to `/usr/bin/oils-for-unix`. Executing `osh` will start a bash-compatible shell, while `ysh` starts Oils with all the feature flags for a modern shell enabled.

| This usage of symlinks to change behaviour is nothing special. When executed via the symlink, the `ARGV[0]` of the executable is set to the name of the symlink. The executable can then use this to change its behaviour. Probably the most famous binary doing this is [busybox](https://busybox.net/), which supports many coreutils tools like `cp`, `ls` and even `grep` and `awk` - all in a single binary. And while we're at it, Busybox also provides the default shell in Alpine Linux that way. The package `busybox-binsh` installs the softlink `/bin/sh -> /bin/busybox`. Oils uses this feature to either start in POSIX, Bash or Ysh "compatibility modes". |

---

`oils-for-unix-binsh` provides the additional "virtual package" called `/bin/sh`, a *path*. This is probably the weirdest package name I've ever seen! Since this path is also all the package provides, the naming makes total sense. The only content is a symlink `/bin/sh -> /usr/bin/oils-for-unix`.

<table style="table-layout: fixed">
  <tbody>
    <tr>
      <td>
          <h2 class="no_toc">What's a virtual package?</h2>
          <span markdown="1">

A "virtual package" is like a group of packages all providing the same or similar function. These packages usually install exactly the same file and therefore can't be installed at the same time. That's the case with `busybox-binsh` and `oils-for-unix-binsh`, which both provide the file (and virtual package!) `/bin/sh`. 

          </span><br />
          <span markdown="1">

For example if I do `apk add /bin/sh` I tell `apk` that I don't care about the exact package, as long as there is some package which *provides* the virtual package `/bin/sh`. It will install `busybox-binsh` because this has the highest `provider_priority` defined ([100](https://gitlab.alpinelinux.org/alpine/aports/-/blob/master/main/busybox/APKBUILD#L328)). My `oils-for-unix-binsh` will be ignored, because it has only a priority of 10:

          </span>
          {% highlight shell %}
# Package definition of oils-for-unix-binsh
binsh() {
	pkgdesc="oils-for-unix as /bin/sh"
	provides="/bin/sh"
	provider_priority=10 # lowest (other providers: dash-binsh, busybox-binsh, yash-binsh)

	mkdir -p "$subpkgdir"/bin
	ln -s /usr/bin/oils-for-unix "$subpkgdir"/bin/sh
}
          {% endhighlight %}
          <span markdown="1">
If I install `/bin/sh` and then run the command `apk add oils-for-unix-binsh` it will first remove `busybox-binsh` and then install `oils-for-unix-binsh`.
          </span>


</td>
</tr>
</tbody>
</table>

---

`oils-for-unix-bash` is like to the `...-binsh` package, except it conflicts with the `bash` package instead of `busybox-binsh`, as it creates a symlink `/bin/bash -> /usr/bin/oils-for-unix`. We could provide a virtual package `/bin/bash` to solve this conflict. But I learned that both packages implicitly provide the virtual package `cmd:bash`. This is because they install an executable file `bash`, and `apk` seems to create a virtual package for each file in `/bin/`.

| I'm honestly not sure why we had to use the `/bin/sh` virtual package previously, when there's already a `cmd:sh` package. I assume historical reasons. |

### Container Volumes

Now remember the last line in the Containerfile?:

```dockerfile
ENTRYPOINT ["/home/packager/aports/oily/container/package.sh"]
```

It points to a script which should be executed when we start the container. This script does only 2 things:

```shell
# oily/container/package.sh

# Make sure the apk signing key is available, and reuse an existing key
../setup-key.sh

# Build all packages
buildrepo -k -l "$HOME/logs" $@ main
buildrepo -k -l "$HOME/logs" $@ community
buildrepo -k -l "$HOME/logs" $@ testing
```

`buildrepo` is a (shell) script written by the Alpine folks to build a whole set of package definitions. But where are these definitions?
And you mightâ€™ve asked yourself, how does the `package.sh` make it into the container?!

I could have an `ADD` statement in the `Containerfile` to copy the `package.sh` into the container image and use `ENTRYPOINT ["/package.sh"]`. But that means whenever I changed the script, I'd have to build a new container image. 

But since I didn't want to copy the whole repository into the container image, I mount the repository into the container at runtime - to the directory `/home/packager/aports` where `buildrepo` expects the package definitions! All the scripts are also in the repository (in the `oily` folder), so we can just use them like `ENTRYPOINT ["/home/packager/aports/oily/container/package.sh"]`.

I added 3 more mount points, one to be able to reuse the same signing-key, one for logs and one for finished packages:

```dockerfile
VOLUME ["/home/packager/aports"]
VOLUME ["/home/packager/.abuild"]
VOLUME ["/home/packager/logs"]
VOLUME ["/home/packager/packages"]
```

And the actual command to run the container looks like this:

```shell
# heavily simplified version of oily/container.sh

cd $GIT_ROOT

package() {
  podman run --rm \
  -v ./:/home/packager/aports \
  -v ./oily/abuild:/home/packager/.abuild \
  -v ./oily/logs:/home/packager/logs \
  -v ./oily/packages:/home/packager/packages \
  oily-pine-builder $@
}
```

In retrospect the additional mount points could just be symlinks like `/home/packager/logs -> /home/packager/aports/oily/logs`.

For a bit of an overview, the file hierarchy of the git repo looks like this:
- oily-pine `# In the container this is /home/packager/aports/`
  - main/ `# package definitions of the main repo`
  - community/ `# package definitions of the community repo`
  - testing/ `# package definitions of the testing repo`
  - oily/
    - container.sh `# Holds functions to build/run the container`
    - container/
      - Containerfile
      - Dockerfile `# A softlink to the Containerfile so it works with Docker as well`
      - setup-build-env.sh
      - package.sh
    - abuild/ `# Keeps the abuild.conf file and key between executions`
    - logs/ `# build logs`
    - packages/ `# finished packages. We could also throw them away...`

## Letâ€™s Build Some Packages

Letâ€™s build and run that Container! I had some issues with IPv4/6, so I told podman to use the host network: 

```shell-session
$ # This runs basically one command:
$ # podman build --network=host oily/container/ -t oily-pine-builder
$ ./oily/container.sh build
...
COMMIT oily-pine-builder
--> 1fed23b134a1
Successfully tagged localhost/oily-pine-builder:latest
1fed23b134a19c77aae25b0591f948d803218cbaa31ada757eb99d3688515ffa
$
$ # This executes the "podman run" from above
$ ./oily/container.sh run
/home/packager
using /home/packager/.abuild/-682737bd.rsa
pigz: not found
1/1579 1/1580 main/acf-jquery 0.4.3-r2
oils: PID 238 exited, but oils didn't start it
oils: PID 258 exited, but oils didn't start it
oils: PID 263 exited, but oils didn't start it
oils: PID 265 exited, but oils didn't start it
oils: PID 261 exited, but oils didn't start it
2/1579 2/1580 main/zlib 1.3.1-r2
oils: PID 594 exited, but oils didn't start it
3/1579 3/1580 main/perl 5.40.2-r0v
4/1579 4/1580 main/linux-headers 6.14.2-r0

...

18/1579 18/1580 main/expat 2.7.1-r0
ERROR: expat: Failed to build
19/1579 18/1580 main/gdbm 1.24-r0

...
 ```

**WEâ€™RE BUILDING!**

Now there were [**A LOT**](https://github.com/Melkor333/oily-pine-logs/blob/master/25-05-16_15%3A04-buildrepo.log) of these `oils: PID xxx exited, but oils didn't start it` warnings, but (most of) the packages built successfully. :)
The above snippet also shows how it looks when the package `expat` fails.

I was so excited about these package builds that I wrote a script which monitored the output and sent me an hourly update to my phone with [ntfy](https://ntfy.sh/):

![Notifications on the phone showing increasing number of packages]({{ site.baseurl }}/assets/images/2025-08-15_11-45-29-oils-notifications.png)
IIRC it was compiling GCC between 00:00-06:00 that day, which just took some time on my very old server.

The builds ran. They had to be restarted a few times due to different issues like an actual infinite loop and I changed it slightly to build main, test and community at the same time. But the number of packages kept growing.

After about 10 days the allocated 500GB disk filled up with 11999 built packages, and 1173 failed Aports. 

| An interesting sidenote: It wasn't the sucessfully built packages filling up the disk! Every failed aport left the build files behind. This could be the unpacked source tarball, generated intermediate files as well as binaries, depending on the package and where it failed. The built `apk`s themselves were only a few GB in size. |

90% success rate in a first attempt isnâ€™t too bad. I hoped for 95% (mainly because it looked like that for the first ~1000 builds), but 90% still seemed OK, given that a single bug hit by `autoconfigure` could affect many packages. Right?

<table style="table-layout: fixed">
  <tbody>
    <tr>
      <td>
          <h2 class="no_toc">WRONG!</h2>

          <span markdown="1">

> ...11999 built *packages*, with 1173 failed *aports*...

I compared apples with oranges:

- Package means "*.apk file"
- Aport means "package definition". 

A single Aport can contain many packages. The Oils Aport I described above has 3 packages, not counting autogenerated debug and doc packages!

I "only" successfully built 5327 Aports out of 6500, which comes down to about 82%, as I realized *while writing this article*.
          </span>
</td>
</tr>
</tbody>
</table>


## Analyze and Categorize

Oblivious of that fact, I started looking into logs trying to find and reproduce actual Oils bugs. I analyzed a few logs and quickly realized that there was an issue.

The process of analyzing a single log file with a new issue requires... TIME! And just from a quick peek into a few log files I was sure weâ€™re hitting the same bugs over and over. If a bug occurs in 100 builds, weâ€™d rather focus on that one than another one happening in a single package build.

So after taking a look at ca. 10 log files, I decided to find a way to quickly *categorize* the 1'173 logs. And what could be better to churn through so much text than a machine? AI! I've never had a real "I *need* AI for that" use case, but this sounded like the perfect opportunity! I'll never be able to look at so many logs myself anyway - or so I thought.

So I whipped up a small script to send the last 40 lines of a logfile to ChatGPT and try to summarize the problem in *maximum 5 words*. Sorry for throwing ysh at you, but it shouldnâ€™t be too hard to understand. Ysh just has a few features which made these 13 lines of code extra nice:

```bash
# simplified categorization-script

#!/usr/bin/env ysh

# *some preparation*

for file in (bad_files => split(u'\n')) {
  ... cat ./PROMPT_TEMPLATE.md
      # Ignore all lines from abuild removing packages at the end
    <(... grep -v "Purging" $file | head -n -1
      # the error should be in the last few lines.
      # chatgpt can't handle too much input!
      | tail -40
      ;)
    | chatgpt
    | tr -d u'\n'
    >> $out_file;

  # Add a newline
  echo >> $out_file
}
````

| the "command" `...` in ysh allows me to spread a single command onto multiple lines without the need of `\` at the end and the possibility to add comments in-between (I love Oils for this!). It just needs a `;` at the end. |

The script joins the `PROMPT_TEMPLATE.md` and 40 lines of the logfile together, sends them to chatgpt and removes any newlines in the chatgpt output (which shouldnâ€™t be more than 5 words anyway!!1!)

Iâ€™m not a prompt Engineer. So I just copied a prompt from a random blog post and adjusted it to fit my need. It's probably heavily overengineered for its usecase:

```markdown
# IDENTITY and CATEGORY
You are a genius error message categorization expert and you are able to understand and categorize errors from software being compiled for Linux.
You specialize in extracting the relevant error messages from a log file and create a very brief category for the error.
Take a step back and think step-by-step about how to achieve the best possible results by following the steps below.

You have no knowledge on what to do with the error message. Your only purpose is to categorize it.

# STEPS
Read the entire log file from an attempted package compilation, trying to identify the exact log line containing the error message which causes the build to fail.
Come up with a description of this error type which is not more than 5 words, to be used as CATEGORY-DESCRIPTION.

# OUTPUT INSTRUCTIONS
Only output a single line of plain text.

Do not give warnings or notes; only output the requested 5 words.
Do not try to give a solution to the error.
You can only output a single sentence of 5 words.
Ensure you follow ALL these instructions when creating your output.

# INPUT
Logs:
```

And it worked (mostly)! :)
I only categorized the ~500 failed packages from the `main` repo to not spend too much money initially. Then I created some stats to see how often Chatgpt used the same "5" words. The top 10 looked like this:

```
    179 c compiler cannot create executables.
     29 phdr segment not covered by load segment.
     13 phdr segment not covered error.
      9 compiler-executable creation error.
      9 c compiler unable to create executables.
      8 c compiler cannot create executables
      7 compiler cannot compile programs.
      7 builddeps failed due to conflicts.
      6 phdr segment not covered by load segment
      6 linker input file not found.
```

Well that was surprising! Apparently there are only 2 major issues, a c compiler issue and a "phdr segment" issue. I tried to reproduce this immediately by building a package by hand. The package built perfectly! I built a bunch of packages by hand without issues, which led me to the conclusion that something must be broken in the container! 

I thought about this for a while and found out that `abuild` has an option called `rootbld`, which builds a single package inside a [bubblewrap sandbox](https://github.com/containers/bubblewrap) (which is more or less a chroot). And `buildrepo` (the tool I use in `package.sh` to build all packages) has a flag to execute each build in a separate sandbox (using `abuild rootbld` under the hood).

Given that Iâ€™ve also seen a few packages fail because they couldnâ€™t spawn a (nested) container for tests, I thought that maybe using the official tooling could solve that issue as well. At the same time, Andy and me discussed the project and how long it took to build the packages initially. He had the idea to use the Github Sponsors money we got over the years for some rented hardware to speed up the process. I investigated a bit and found that a [Hetzner EX44](https://www.hetzner.com/dedicated-rootserver/ex44/configurator/#/) has a super price/performance ratio and matches the budget/requirements very well ([comparison to the processor of the initial build](https://www.cpubenchmark.net/compare/4993vs1191/Intel-i5-13500-vs-Intel-Xeon-E3-1245-V2)).

## Alpine Linux + Rootbld

To potentially fix both the nested container and the phdr/c compiler issues - and to lean a bit more into Alpine upstream tooling - I decided to create a big Alpine Linux VM when I got access to the physical Server. And so I began getting ready to build packages with Oils in a bubblewrap chroot directly on Alpine.

Configuring the VM itself didnâ€™t require much more than running the `setup-build-env.sh` script, moving the repo to `/home/packager/aports` and this time creating softlinks for the directories I used to mount into the container.

### Adjusting Abuild

Abuild sets up a chroot directory by installing a hardcoded list of packages into a temporary directory.
This list needed to be adjusted to install the Oils packages as well:


```shell
# abuild command to prepare the build environment

echo "installing oils!"
$SUDO_APK add --initdb --update \
      --no-interactive \
      --arch $CBUILD_ARCH \
      --root "$BUILD_ROOT" \
      ${cachedir:+--cache-dir $cachedir} \
      abuild alpine-base build-base git $hostdeps $builddeps oils-for-unix-binsh \
      ${USE_CCACHE:+ccache}
```

All I had to do was add `oils-for-unix-binsh`. Thatâ€™s it!

Notice that this time I didnâ€™t install `oils-for-unix` or `oils-for-unix-bash`. `oils-for-unix-binsh` depends on `oils-for-unix`, so I donâ€™t need to explicitly specify that anyway. And I decided to not replace `/bin/bash` *yet*...

### Stonks?!

...because Andy "secured" another NLNet Grant!

| The [NLNet Foundation](https://nlnet.nl/) has various programs to fund projects that contribute to an open internet for all.<br />The Oils project has already been able to vastly improve different parts with 3 prior funds. For example the garbage collector built into Oils was mostly funded with the second grant. And the last grant was used to make the docs much more usable, as well as adding a lot more spec tests (and builtins) for ysh. |

In the second quarter of this year, Andy filled in the form for the fourth grant with the objective of stabilizing the Shell/Bash compatible part of the project. And believing in my work, he wanted to base it off of this project! ðŸ¤©

The idea is to use the grant to find and fix bugs - with the help of oily-pine!
The "event loop" is basically:
- analyze why package X doesnâ€™t build
- reduce the bug to a simple spec test
- fix it

BTW. Iâ€™m terrible at actually fixing bugs in the Oils codebase. Iâ€™m a Systems Engineer, not a Developer! But luckily, there are people like Andy who can do the coding. It's already useful when I only analyze the bugs. :)

To keep the scope reasonable, we decided to only focus on the `main` repo in the beginning - and only replace `/bin/sh`. We do think POSIX is a good and realistic target. It's easy to extend the the scope to more packages and Bash later on.

| We also do hope to attract more contributors with the grant. If **you** are interested, please hit us up on [zulip](https://oilshell.zulipchat.com/)! :) |

## Categorical

I digressed... With `oils-for-unix-binsh` added to the package list, `abuild rootbld` just works as expected. The only thing left was to run the command `buildrepo -k -l "$HOME/logs" main`.

This time I successfully built 1365/1580 packages (215 failures) in not even a day! The new server was really worth it. The 86% success rate is actually still not much, but with the grant in place I actually never even calculated the number until writing this article.

Now that the failed package count wasn't over 1000 anymore and I wanted to name some known bugs correctly, I went for manually categorizing the failed packages. I wrote yet another [small script](https://github.com/Melkor333/oily-pine/blob/9cb57f0efc7a96d4c98e337f6d7f9802c57ca419/oily/logs.ysh#L49) which showed me the error log, gave me an [FZF prompt](https://junegunn.github.io/fzf/) to select an existing category and when I pressed `ctrl+c` it asked for a new category description. In only around 2 hours of manual work the second categorization was done!

| Fun fact: I did most of the categorizations on the phone using [JuiceSSH](https://juicessh.com/) when I was in the train, on the toilet or walking my baby to sleep in the baby carrier. Not only that, but most of the work for this whole project was actually done on my phone! I have several projects that only exist inside of a [Termux](https://termux.dev/en/) terminal. |

I created a repository to keep the logs, you can find them [on github](https://github.com/Melkor333/oily-pine-logs).
And this is how the summary looks like:

```
     59 ifs=\\
     39 broken testsuite
     25 cmake error - compatibility unsupported
     15 libtool invalid word while parsing
     11 mkdir: unrecognized option: /
     10 config.status compile error
     10 abuild dependency issue (e.g. depends on busybox-binsh)
      9 abuild - fetch source issue
      6 (( .. ) .. )
      5 atf-test-program fail
      4 gpgrt-config timeout
      3 OSH command not found
      2 missing header file
      1 unexpected rustc version (alpine issue?)
      1 print oils ast error
      1 osh printf doesn't support single characters
      1 ninja build failure
      1 missing FNM_EXTMATCH support in libc
      1 meson test - test must be compiled first
      1 meson test - file not found
      1 libtool unrecognized macro
      1 ifs=\
      1 couldn't fetch source during build
      1 configure: error: can't find a separator character in '+,;&' for the path_replacer shell script
      1 configure.ac terminate called after throwing an instance of 'IndexError*'
      1 config.status (test) Unexpected trailing word '--'
      1 compile time error
      1 abuild - cant fetch package index
      1 Makefile Error 1
      1 'trap' requires a signal or hook name
215 /home/packager/oily-pine/oily/logs/issues
```

Now we have a list of 30 rough categories of bugs with a useful priorisation to look at! ðŸ¥³ðŸŽ‰ðŸ¥³ðŸŽ‰ðŸ¥³ðŸŽ‰

## Epilogue

We only built a fraction of all the packages, and there are still a lot of build failures.
But it's enough to prove the idea works. The next step is now to turn these failing packages into test cases and fix them.
You'll definitely get more updates on the [Oils Blog](https://oils.pub/blog/). Because besides fixing bugs, writing or talking about the work done is also an important part of the NLNet grant. We're planning to publish "Bug Post Mortems", writing about the journey from failed package to fixed bug.

### Plot-Twist

One last thing happened: Andy mostly rebuilt the packaging logic a *third time* directly in the Oils repository, for 2 Reasons:
- He didn't want to depend on an Alpine VM, thus using `abuild rootbld` directly doesn't work. But his tool only uses an Alpine *chroot*.
- He wanted to trigger the builds in a pipeline and generate HTML tables
   - His script fetches all the source tarballs in a perparation step. Thus downloading these files isn't required anymore (oily-pine fetches them every time)
   - One thing I haven't even mentioned is a "baseline build" using `busybox ash`. Because it turns out some packages don't even build with a stock build environment. The logs of these packages are irrelevant to us for the moment. And apparently there are *over a hundred* build failures in the baseline build, which is crazy. There is now an additional task in our todolist to reduce these baseline build failures!
   - There needed to be various extra scripts to generate useful HTML output from the CI.

I was very glad that he spent the 2 weeks of effort, because now we have a very nice and short dashboard with only the packages we should really focus on. 

With some bugs fixed (e.g. the top issue `ifs=\\`) and failing baseline packages removed, we're currently at only **62 relevant packages** as you can see in the current [Dasboard](https://op.oils.pub/aports-build/2025-09-06-edit.wwz/_tmp/aports-report/2025-09-06-edit/diff_merged.html) (as of writing). We might do an extension to Bash and/or the community packages sooner than I thought. ðŸ¤©

You might notice that (at the time of writing...) the dashboard doesn't contain my categorizations... Putting these into the repository will be my next task after publishing this article. So you might already see them - or not anymore, because we fixed them all! ðŸ˜›

| Thanks a lot for your interest! And once again, if you got interested in the project, we're very glad for *any* feedback. Be it a [pull request](https://github.com/oils-for-unix/oils/pulls), a [bug report](https://github.com/oils-for-unix/oils/issues] or just an introduction on [Zulipchat](https://oilshell.zulipchat.com/#narrow/channel/119655-new-members) to get to know us and the project :) |
